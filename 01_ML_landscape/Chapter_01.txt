01)

Machine Learning is a subset of artificial intelligence that 
allows systems to automatically imporove their performance on a 
specific task through experience without being explicitly programmed.
The systems learn from input data and make predictions or take 
actions based on that input data.

-----------------------------------------------------------
02)

Classification: 
Predicting which category an item belongs to, 
based on labeled training data. Ex.: Picture of Dog/Cat

Regression: 
Predicting a continuous target variable based on one or 
more input variables. Ex: Estimating house price based on 
realstate history.

Clustering: 
Dividing a set of objects intro groups based
on their siilarity or distance. Ex: Flower Picture Classification

Anomaly Detection:
Identifying items or events that are unusual or deviate from
the norm in some way.

-----------------------------------------------------------
03)

A labeled training set is a dataset used to train a machine 
learning model, where each data instance (sample) is labeled
with the correct output or class. The purpose of using a labeled 
training set is to teach the model to make correct predictions
by using the correct labels as ground truth. The model adjusts
its internal parameters based one the observed training data
and the corresponding labels, and hopefully generalizes well
to unseen data. The labeled training set is usually a portion
of the available data, selected for the purpose of learning a 
model, while the remaining data is used for testing the
model's performance.
-----------------------------------------------------------
04)

Classification: 
a supervised learning task in which the model learns to predict
the class or category of a given data sample based on labeled 
training data.

Regression:
A supervised learning task in which the model learns to predict 
a continuous target variable on one or more input variables. 
The goal is to find the mapping between the input and output
variables.

-----------------------------------------------------------

05)

Clustering:
grouping similar data points together into clusters

Dimensionality reduction:
reducing the number of features in a dataset while retaining
as much of the original information as possible

Anomaly detection:
Identifying data points that are different from the majority
of the data

Association rule learning:
finding rules that describe the relationships between variables
in a dataset

-----------------------------------------------------------

06)

Reinforcement learning would be a suitable type of machine learning
algorithm for allowing a robot to walk in various unknown terrains.
Reinforcement learning is a type of machine learning where an agent
learns to make decisions by performing actions in an environment
to maximize a reward signal. In the casa of a walking robot, the
environment would be the terrainm the actions would be the 
movements of the robot, and the reward signal could be based on 
successfull steps taken or some other performance metric. The robot
would learn through trial and error, gradually improving its 
walking abilities as it receives more information about the 
terrain.

-----------------------------------------------------------

07)

A common type of machine learning algorithm used to segment 
costumers into multiple groups is clustering. Clustering is an
unsupervised learning task where the goal is to divide a set 
of objects into groups (clusters) based on their similarity
or distance. This can be applied to costumer data by using variables
such as demographics, purchasing behavior, etc. The algorithm
will then identify patterns in the data and group similar 
costumers together, allowing for segmentation into distinct 
costumer groups. Some popular clustering algorithms include
K-Means, Hierarchical Clustering and DBSCAN.

-----------------------------------------------------------

08)

supervised learning. The model works on pre-labeled emails 
with flags: spam or ham. 

-----------------------------------------------------------

09)

An online learning system is a type of machine learning system
where the model learns incrementally as new data is made availabe,
rather than being trained on all the data in batch mode. 
Ex: Streaming data for sensors, stock prices and web logs.

-----------------------------------------------------------

10)

Out-of-core learning is a type of machine learning that allows 
the training of models on datasets that are too large to fit 
into the memory of a single machine. This is achieved by processing 
the data in chunks, with each chunk being processed and the model
updated incrementally. The process continues until the entire 
dataset has been processed.


-----------------------------------------------------------

11)

instance-based learning or similarity-based learning. Knn


-----------------------------------------------------------

12)

A model parameter is a value that is learned by the machine 
learning algorithm during the training process, based on the 
input data. Model parameters define the structure of the model
 and its behavior, and they are used to make predictions on new 
 data instances.

 On the other hand, a hyperparameter is a value that is set by the 
 user before training the model and controls the learning algorithmâ€™s 
 behavior. Hyperparameters are not learned from the data and they 
 determine the overall performance of the model. Examples of 
 hyperparameters include the learning rate, the number of trees 
 in a random forest, the number of clusters in a K-Means algorithm, 
 and the regularization strength in linear regression.

-----------------------------------------------------------

13)

Model-based learning algorithms search for the best model that 
represents the relationships between the input features and the 
target variable. The most common strategy used by model-based 
learning algorithms is to minimize the difference between the 
predicted values and the actual values of the target variable, 
as measured by a loss function.

The most common strategy used by model-based learning algorithms
 to minimize the loss function is optimization. This involves 
 finding the values of the model parameters that minimize the loss 
 function, usually by using an optimization algorithm such as 
 gradient descent, conjugate gradient, or quasi-Newton methods.

-----------------------------------------------------------

14)

Overfitting: 
Overfitting occurs when a model learns the training 
data too well and becomes overly complex, capturing noise and 
random fluctuations in the data. This leads to poor performance 
on new, unseen data.

Underfitting: 
Underfitting occurs when a model is too simple and doesn't capture 
the underlying relationships in the data. This leads to poor 
performance on both the training and test data.

Lack of interpretability: 
Many machine learning models are highly complex and difficult to 
interpret, making it challenging to understand how the model is 
making predictions and why certain decisions are being made.

Imbalanced data: 
In many real-world problems, the data is imbalanced,
with some classes being much more prevalent than others. 
This can lead to biased models that do not perform well on the 
under-represented classes.

These challenges can impact the performance and effectiveness of
machine learning models and require careful consideration and 
techniques to overcome, such as regularization, ensembling, and 
resampling techniques for imbalanced data.

-----------------------------------------------------------

15)

Overfitting

1 -
Regularization: Regularization is a technique that adds a penalty 
term to the loss function to prevent the model from becoming too 
complex. This helps to reduce overfitting and improve generalization
performance. Examples of regularization methods include L1 and L2 
regularization for linear models and dropout for neural networks.

2 -
Ensemble methods: Ensemble methods involve combining the predictions
of multiple models to make a final prediction. This can help to 
reduce overfitting by averaging out the noise and random 
fluctuations in the individual models. Examples of ensemble methods
include random forests and gradient boosting.

3 - 
Early stopping: Early stopping is a technique that stops the 
training process before the model becomes too complex. This can 
be done by monitoring the performance of the model on a validation 
set and stopping the training process when the performance on the 
validation set starts to degrade. This helps to prevent overfitting 
and improve generalization performance.

In addition to these solutions, reducing the complexity of the model,
such as by using a simpler model structure or a smaller number of 
features, can also help to reduce overfitting and improve 
generalization performance.

-----------------------------------------------------------

16)

A test set is a portion of the data set that is set aside for the
purpose of evaluating the performance of a machine learning model.
It is used to assess the ability of the model to generalize to 
new, unseen data.

It is important to use a test set because overfitting is a common 
problem in machine learning, and it can be difficult to determine 
the true performance of a model based solely on its accuracy on the
training set.


-----------------------------------------------------------

17)

A validation set is a portion of the data set that is set aside 
for the purpose of evaluating the performance of a machine learning
model during the training process. It is used to tune the 
hyperparameters of the model and to prevent overfitting.

-----------------------------------------------------------

18)

A train-dev set, also known as a development set, is a portion of 
the data set that is set aside for the purpose of evaluating the
performance of a machine learning model during the training 
process. It is used to monitor the model's performance and to 
detect overfitting.


-----------------------------------------------------------

19)

Tuning hyperparameters using the test set can result in overfitting 
to the test set, which can lead to overly optimistic and misleading 
estimates of the model's performance on new, unseen data.

-----------------------------------------------------------







